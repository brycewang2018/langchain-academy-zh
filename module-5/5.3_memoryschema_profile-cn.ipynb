{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 使用 Profile Schema 的 Chatbot\n\n## 回顾\n\n我们介绍了 [LangGraph Memory Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore),这是一种保存和检索长期记忆的方式。\n\n我们构建了一个简单的 chatbot,它同时使用 `短期记忆(线程内)` 和 `长期记忆(跨线程)`。\n\n它将长期 [语义记忆](https://langchain-ai.github.io/langgraph/concepts/memory/#semantic-memory)(关于用户的事实)保存在[\"热路径\"](https://langchain-ai.github.io/langgraph/concepts/memory/#writing-memories)中,即在用户与其聊天时实时保存。\n\n## 目标\n\n我们的 chatbot 将记忆保存为字符串。在实践中,我们通常希望记忆具有结构化的形式。\n \n例如,记忆可以是一个 [单一的、持续更新的 schema]((https://langchain-ai.github.io/langgraph/concepts/memory/#profile))。\n \n在我们的案例中,我们希望这是一个单一的用户 profile。\n \n我们将扩展我们的 chatbot,将语义记忆保存到单一的 [用户 profile](https://langchain-ai.github.io/langgraph/concepts/memory/#profile) 中。\n\n我们还将介绍一个库,[Trustcall](https://github.com/hinthornw/trustcall),用于使用新信息更新这个 schema。"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain_openai langgraph trustcall langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, getpass\n\ndef _set_env(var: str):\n    \"\"\"\n    辅助函数 - 设置环境变量\n    \n    Python 知识点:\n    - os.environ: Python 的环境变量字典,可以读取和设置系统环境变量\n    - getpass.getpass(): 安全地提示用户输入密码,输入时不会显示在屏幕上\n    \n    工作流程:\n        1. 检查环境变量是否已设置\n        2. 如果未设置,提示用户输入\n        3. 将值设置到当前进程的环境变量中\n    \"\"\"\n    # 检查环境变量是否已在 OS 环境中设置\n    env_value = os.environ.get(var)\n    if not env_value:\n        # 如果未设置,提示用户输入\n        env_value = getpass.getpass(f\"{var}: \")\n    \n    # 为当前进程设置环境变量\n    os.environ[var] = env_value\n\n# ================== LangSmith 配置 ==================\n# LangSmith: LangChain 的追踪和调试平台\n# 用于监控和调试 LangChain 应用的运行情况\n\n# 设置 LangSmith API 密钥\n_set_env(\"LANGSMITH_API_KEY\")\n\n# 启用 LangSmith 追踪功能\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\n\n# 设置 LangSmith 项目名称,所有追踪数据将保存到此项目\nos.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 定义用户 profile schema\n\nPython 有许多不同的 [结构化数据](https://python.langchain.com/docs/concepts/structured_outputs/#schema-definition) 类型,例如 TypedDict、Dictionaries、JSON 和 [Pydantic](https://docs.pydantic.dev/latest/)。\n\n让我们从使用 TypedDict 定义用户 profile schema 开始。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from typing import TypedDict, List\n\n# ================== 定义用户 Profile Schema ==================\n\nclass UserProfile(TypedDict):\n    \"\"\"\n    用户 profile schema - 使用 TypedDict 定义\n    \n    Python 知识点:\n    - TypedDict: Python 3.8+ 引入的类型提示工具\n    - 它定义了字典的结构,指定每个键的类型\n    - 与普通 dict 不同,TypedDict 提供类型检查支持\n    - 运行时仍然是普通字典,但 IDE 和类型检查器可以验证类型\n    \n    为什么使用 TypedDict:\n    - 轻量级:比 Pydantic 更简单,适合简单的数据结构\n    - 类型安全:提供类型提示,帮助捕获错误\n    - 兼容性:可以直接作为普通字典使用\n    \n    Schema 字段说明:\n    - user_name: 用户的首选名称\n    - interests: 用户的兴趣爱好列表\n    \"\"\"\n    user_name: str  # 用户的首选名称\n    interests: List[str]  # 用户的兴趣爱好列表"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 将 schema 保存到 store\n\n[LangGraph Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) 接受任何 Python 字典作为 `value`。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================== 创建 TypedDict 实例 ==================\n\n# Python 知识点:\n# - TypedDict 实例实际上就是普通的 Python 字典\n# - 但类型检查器会验证字典的结构是否符合 TypedDict 定义\n# - 运行时没有额外的开销,只是提供了类型安全\n\n# 创建符合 UserProfile schema 的字典实例\nuser_profile: UserProfile = {\n    \"user_name\": \"Lance\",  # 字符串类型\n    \"interests\": [\"biking\", \"technology\", \"coffee\"]  # 字符串列表\n}\n\n# 显示创建的 profile\nuser_profile"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "我们使用 [put](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.put) 方法将 TypedDict 保存到 store。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import uuid\nfrom langgraph.store.memory import InMemoryStore\n\n# ================== 初始化 Store 并保存 Schema ==================\n\n# LangGraph 知识点:\n# - InMemoryStore: LangGraph 的内存存储实现\n# - 用于保存和检索长期记忆(跨线程)\n# - 数据保存在内存中,进程重启后会丢失\n# - 生产环境可以替换为持久化的 Store 实现\n\n# 初始化内存 store\nin_memory_store = InMemoryStore()\n\n# ================== 定义 Namespace ==================\n# Namespace: Store 中的命名空间,用于组织和隔离数据\n# 格式: tuple 类型,通常为 (user_id, memory_type)\n# 作用: \n# - 为每个用户创建独立的存储空间\n# - 避免不同用户的数据混淆\n\nuser_id = \"1\"\nnamespace_for_memory = (user_id, \"memory\")\n\n# ================== 保存记忆到 Store ==================\n# Store 的 put 方法参数:\n# - namespace: 命名空间,用于组织数据\n# - key: 记忆的唯一标识符\n# - value: 要保存的数据(任何 Python 字典)\n\nkey = \"user_profile\"\nvalue = user_profile\n\n# 将 user_profile 保存到 store\n# Python 知识点: TypedDict 实例本质上是普通字典,可以直接保存\nin_memory_store.put(namespace_for_memory, key, value)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "我们使用 [search](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.search) 通过 namespace 从 store 检索对象。"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': {'user_name': 'Lance', 'interests': ['biking', 'technology', 'coffee']}, 'key': 'user_profile', 'namespace': ['1', 'memory'], 'created_at': '2024-11-04T23:37:34.871675+00:00', 'updated_at': '2024-11-04T23:37:34.871680+00:00'}\n"
     ]
    }
   ],
   "source": [
    "# Search \n",
    "for m in in_memory_store.search(namespace_for_memory):\n",
    "    print(m.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "我们也可以使用 [get](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.get) 通过 namespace 和 key 检索特定对象。"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_name': 'Lance', 'interests': ['biking', 'technology', 'coffee']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the memory by namespace and key\n",
    "profile = in_memory_store.get(namespace_for_memory, \"user_profile\")\n",
    "profile.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 使用 profile schema 的 Chatbot\n\n现在我们知道如何为记忆指定 schema 并将其保存到 store。\n\n现在,我们如何实际*创建*具有这个特定 schema 的记忆?\n\n在我们的 chatbot 中,我们[希望从用户聊天中创建记忆](https://langchain-ai.github.io/langgraph/concepts/memory/#profile)。\n\n这就是 [structured outputs](https://python.langchain.com/docs/concepts/structured_outputs/#recommended-usage) 概念发挥作用的地方。\n\nLangChain 的 [chat model](https://python.langchain.com/docs/concepts/chat_models/) 接口有一个 [`with_structured_output`](https://python.langchain.com/docs/concepts/structured_outputs/#recommended-usage) 方法来强制执行结构化输出。\n\n当我们想要强制输出符合 schema 时,这很有用,它会为我们解析输出。"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "让我们将我们创建的 `UserProfile` schema 传递给 `with_structured_output` 方法。\n\n然后我们可以使用一个 [messages](https://python.langchain.com/docs/concepts/messages/) 列表调用 chat model,并获得符合我们 schema 的结构化输出。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pydantic import BaseModel, Field\n\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\n\n# ================== 使用 with_structured_output ==================\n\n# LangChain 知识点:\n# - with_structured_output: LangChain chat model 的方法\n# - 作用: 强制 LLM 输出符合指定 schema 的结构化数据\n# - 底层: 使用 tool calling 或 JSON mode 实现\n# - 优势: 自动解析和验证输出,确保类型安全\n\n# 初始化 model\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n\n# 将 schema 绑定到 model\n# Python 知识点:\n# - TypedDict 可以直接传递给 with_structured_output\n# - model_with_structure 是一个新的 Runnable,返回类型为 UserProfile\nmodel_with_structure = model.with_structured_output(UserProfile)\n\n# ================== 调用 Model 生成结构化输出 ==================\n# 工作流程:\n# 1. 将 HumanMessage 传递给 model\n# 2. Model 理解消息内容并提取信息\n# 3. Model 生成符合 UserProfile schema 的输出\n# 4. with_structured_output 自动解析和验证输出\n\n# 创建 HumanMessage\n# LangChain 知识点: HumanMessage 表示用户发送的消息\nmessage = HumanMessage(\"My name is Lance, I like to bike.\")\n\n# 调用 model 并获取结构化输出\n# 返回值: 符合 UserProfile schema 的字典\nstructured_output = model_with_structure.invoke([message])\n\n# 显示结果\nstructured_output"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "现在,让我们在我们的 chatbot 中使用它。\n\n这只需要对 `write_memory` 函数进行少量修改。\n\n我们使用如上定义的 `model_with_structure` 来生成符合我们 schema 的 profile。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from IPython.display import Image, display\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph, MessagesState, START, END\nfrom langgraph.store.base import BaseStore\n\nfrom langchain_core.messages import HumanMessage, SystemMessage, AIMessage\nfrom langchain_core.runnables.config import RunnableConfig\n\n# ================== Chatbot 系统提示词 ==================\n\n# Chatbot 指令 - 定义 chatbot 的行为和个性\nMODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful assistant with memory that provides information about the user. \nIf you have memory for this user, use it to personalize your responses.\nHere is the memory (it may be empty): {memory}\"\"\"\n\n# 记忆创建指令 - 指导 LLM 如何从对话中创建/更新记忆\nCREATE_MEMORY_INSTRUCTION = \"\"\"Create or update a user profile memory based on the user's chat history. \nThis will be saved for long-term memory. If there is an existing memory, simply update it. \nHere is the existing memory (it may be empty): {memory}\"\"\"\n\n# ================== call_model 节点 ==================\n\ndef call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n    \"\"\"\n    主对话节点 - 从 store 加载记忆并生成个性化响应\n    \n    LangGraph 知识点:\n    - 这是 StateGraph 的一个节点函数\n    - 节点函数签名: (state, config, store) -> dict\n    - 返回的字典会更新 state\n    \n    工作流程:\n        1. 从 config 获取 user_id\n        2. 从 store 检索该用户的记忆\n        3. 将记忆格式化并注入系统提示\n        4. 使用记忆和对话历史生成响应\n    \n    参数说明:\n    - state: MessagesState,包含对话历史 messages\n    - config: RunnableConfig,包含配置信息(如 user_id, thread_id)\n    - store: BaseStore,长期记忆存储\n    \"\"\"\n\n    # ========== 1. 获取 user_id ==========\n    # Python 知识点: config[\"configurable\"] 是一个字典,存储用户自定义配置\n    user_id = config[\"configurable\"][\"user_id\"]\n\n    # ========== 2. 从 store 检索记忆 ==========\n    # LangGraph Store 知识点:\n    # - namespace: (memory_type, user_id) 组织数据\n    # - get(namespace, key): 检索特定记忆\n    # - 返回 Item 对象或 None\n    namespace = (\"memory\", user_id)\n    existing_memory = store.get(namespace, \"user_memory\")\n\n    # ========== 3. 格式化记忆 ==========\n    # Python 知识点:\n    # - existing_memory.value: 获取存储的字典数据\n    # - dict.get(key, default): 安全地获取字典值,不存在时返回默认值\n    if existing_memory and existing_memory.value:\n        memory_dict = existing_memory.value\n        # 将字典格式化为人类可读的字符串\n        formatted_memory = (\n            f\"Name: {memory_dict.get('user_name', 'Unknown')}\\n\"\n            f\"Interests: {', '.join(memory_dict.get('interests', []))}\"\n        )\n    else:\n        # 如果没有记忆,设置为 None\n        formatted_memory = None\n\n    # ========== 4. 注入记忆到系统提示 ==========\n    # Python 字符串格式化: .format() 方法替换 {memory} 占位符\n    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=formatted_memory)\n\n    # ========== 5. 生成响应 ==========\n    # LangChain 知识点:\n    # - model.invoke(): 调用 LLM 生成响应\n    # - 输入: [SystemMessage, ...历史消息]\n    # - SystemMessage 在前,提供上下文和指令\n    # - state[\"messages\"] 包含完整的对话历史\n    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n\n    # ========== 6. 返回更新 ==========\n    # LangGraph 知识点:\n    # - 返回字典会与 state 合并\n    # - {\"messages\": response} 将新消息添加到 state[\"messages\"]\n    return {\"messages\": response}\n\n# ================== write_memory 节点 ==================\n\ndef write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n    \"\"\"\n    记忆写入节点 - 反思对话并保存结构化记忆到 store\n    \n    关键改进:\n    - 使用 with_structured_output 强制输出符合 UserProfile schema\n    - 从头开始重新生成整个 profile(后续会改进为增量更新)\n    \n    工作流程:\n        1. 从 config 获取 user_id\n        2. 从 store 检索现有记忆\n        3. 格式化现有记忆为提示\n        4. 使用 model_with_structure 生成新的结构化 profile\n        5. 覆盖保存到 store\n    \n    局限性:\n    - 每次从头重新生成整个 profile,可能丢失信息\n    - 对于大型 profile,浪费 tokens\n    - 后续会使用 Trustcall 改进为增量更新\n    \"\"\"\n    \n    # ========== 1. 获取 user_id ==========\n    user_id = config[\"configurable\"][\"user_id\"]\n\n    # ========== 2. 检索现有记忆 ==========\n    namespace = (\"memory\", user_id)\n    existing_memory = store.get(namespace, \"user_memory\")\n\n    # ========== 3. 格式化现有记忆 ==========\n    if existing_memory and existing_memory.value:\n        memory_dict = existing_memory.value\n        formatted_memory = (\n            f\"Name: {memory_dict.get('user_name', 'Unknown')}\\n\"\n            f\"Interests: {', '.join(memory_dict.get('interests', []))}\"\n        )\n    else:\n        formatted_memory = None\n        \n    # ========== 4. 创建提示 ==========\n    system_msg = CREATE_MEMORY_INSTRUCTION.format(memory=formatted_memory)\n\n    # ========== 5. 生成结构化 profile ==========\n    # LangChain 知识点:\n    # - model_with_structure: 之前用 with_structured_output 绑定的 model\n    # - 输入: [SystemMessage, ...对话历史]\n    # - 输出: 符合 UserProfile schema 的字典\n    # - 自动解析和验证输出\n    new_memory = model_with_structure.invoke([SystemMessage(content=system_msg)]+state['messages'])\n\n    # ========== 6. 覆盖保存记忆 ==========\n    # Store 知识点:\n    # - put(namespace, key, value): 覆盖式保存\n    # - 如果 key 已存在,会完全替换旧值\n    # - value 必须是 Python 字典\n    key = \"user_memory\"\n    store.put(namespace, key, new_memory)\n\n# ================== 构建 Graph ==================\n\n# LangGraph 知识点:\n# - StateGraph: 定义状态机的图结构\n# - MessagesState: 内置状态类,包含 messages 列表\n# - 图由节点(nodes)和边(edges)组成\n\n# 定义图\nbuilder = StateGraph(MessagesState)\n\n# 添加节点\n# - call_model: 主对话节点\n# - write_memory: 记忆写入节点\nbuilder.add_node(\"call_model\", call_model)\nbuilder.add_node(\"write_memory\", write_memory)\n\n# 添加边\n# LangGraph 知识点:\n# - add_edge(from, to): 添加有向边\n# - START: 图的起始点\n# - END: 图的终止点\n# - 执行流程: START -> call_model -> write_memory -> END\nbuilder.add_edge(START, \"call_model\")\nbuilder.add_edge(\"call_model\", \"write_memory\")\nbuilder.add_edge(\"write_memory\", END)\n\n# ================== 初始化双记忆系统 ==================\n\n# 长期记忆 (跨线程) - 使用 Store\n# - 保存用户 profile\n# - 不同线程间共享\n# - 按 user_id 组织\nacross_thread_memory = InMemoryStore()\n\n# 短期记忆 (线程内) - 使用 Checkpointer\n# - 保存对话历史\n# - 每个线程独立\n# - 按 thread_id 组织\nwithin_thread_memory = MemorySaver()\n\n# ================== 编译 Graph ==================\n\n# LangGraph 知识点:\n# - compile(): 编译图为可执行的 Runnable\n# - checkpointer: 启用短期记忆(对话历史)\n# - store: 启用长期记忆(用户 profile)\n# - 两个记忆系统协同工作,提供完整的记忆功能\ngraph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n\n# 可视化图结构\n# xray=1: 展开节点内部细节\ndisplay(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, my name is Lance and I like to bike around San Francisco and eat at bakeries.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Lance! It's great to meet you. Biking around San Francisco sounds like a fantastic way to explore the city, and there are so many amazing bakeries to try. Do you have any favorite bakeries or biking routes in the city?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Lance and I like to bike around San Francisco and eat at bakeries.\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "让我们检查 store 中的记忆。\n\n我们可以看到记忆是一个符合我们 schema 的字典。"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_name': 'Lance', 'interests': ['biking', 'bakeries', 'San Francisco']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace = (\"memory\", user_id)\n",
    "existing_memory = across_thread_memory.get(namespace, \"user_memory\")\n",
    "existing_memory.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 什么时候会失败?\n\n[`with_structured_output`](https://python.langchain.com/docs/concepts/structured_outputs/#recommended-usage) 非常有用,但如果我们处理更复杂的 schema 会发生什么?\n\n[这里](https://github.com/hinthornw/trustcall?tab=readme-ov-file#complex-schema)是一个更复杂的 schema 示例,我们将在下面测试。\n\n这是一个 [Pydantic](https://docs.pydantic.dev/latest/) model,描述了用户对通信和 trust fall 的偏好。"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "class OutputFormat(BaseModel):\n",
    "    preference: str\n",
    "    sentence_preference_revealed: str\n",
    "\n",
    "class TelegramPreferences(BaseModel):\n",
    "    preferred_encoding: Optional[List[OutputFormat]] = None\n",
    "    favorite_telegram_operators: Optional[List[OutputFormat]] = None\n",
    "    preferred_telegram_paper: Optional[List[OutputFormat]] = None\n",
    "\n",
    "class MorseCode(BaseModel):\n",
    "    preferred_key_type: Optional[List[OutputFormat]] = None\n",
    "    favorite_morse_abbreviations: Optional[List[OutputFormat]] = None\n",
    "\n",
    "class Semaphore(BaseModel):\n",
    "    preferred_flag_color: Optional[List[OutputFormat]] = None\n",
    "    semaphore_skill_level: Optional[List[OutputFormat]] = None\n",
    "\n",
    "class TrustFallPreferences(BaseModel):\n",
    "    preferred_fall_height: Optional[List[OutputFormat]] = None\n",
    "    trust_level: Optional[List[OutputFormat]] = None\n",
    "    preferred_catching_technique: Optional[List[OutputFormat]] = None\n",
    "\n",
    "class CommunicationPreferences(BaseModel):\n",
    "    telegram: TelegramPreferences\n",
    "    morse_code: MorseCode\n",
    "    semaphore: Semaphore\n",
    "\n",
    "class UserPreferences(BaseModel):\n",
    "    communication_preferences: CommunicationPreferences\n",
    "    trust_fall_preferences: TrustFallPreferences\n",
    "\n",
    "class TelegramAndTrustFallPreferences(BaseModel):\n",
    "    pertinent_user_preferences: UserPreferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "现在,让我们尝试使用 `with_structured_output` 方法提取这个 schema。"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 validation error for TelegramAndTrustFallPreferences\n",
      "pertinent_user_preferences.communication_preferences.semaphore\n",
      "  Input should be a valid dictionary or instance of Semaphore [type=model_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    }
   ],
   "source": [
    "from pydantic import ValidationError\n",
    "\n",
    "# Bind schema to model\n",
    "model_with_structure = model.with_structured_output(TelegramAndTrustFallPreferences)\n",
    "\n",
    "# Conversation\n",
    "conversation = \"\"\"Operator: How may I assist with your telegram, sir?\n",
    "Customer: I need to send a message about our trust fall exercise.\n",
    "Operator: Certainly. Morse code or standard encoding?\n",
    "Customer: Morse, please. I love using a straight key.\n",
    "Operator: Excellent. What's your message?\n",
    "Customer: Tell him I'm ready for a higher fall, and I prefer the diamond formation for catching.\n",
    "Operator: Done. Shall I use our \"Daredevil\" paper for this daring message?\n",
    "Customer: Perfect! Send it by your fastest carrier pigeon.\n",
    "Operator: It'll be there within the hour, sir.\"\"\"\n",
    "\n",
    "# Invoke the model\n",
    "try:\n",
    "    model_with_structure.invoke(f\"\"\"Extract the preferences from the following conversation:\n",
    "    <convo>\n",
    "    {conversation}\n",
    "    </convo>\"\"\")\n",
    "except ValidationError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "如果我们天真地提取更复杂的 schema,即使使用像 `gpt-4o` 这样的高容量模型,也容易失败。"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 使用 Trustcall 创建和更新 profile schemas\n\n正如我们所看到的,处理 schemas 可能很棘手。\n\n复杂的 schemas 可能难以提取。\n\n此外,即使是简单的 schemas,更新也会带来挑战。\n\n考虑我们上面的 chatbot。\n\n我们每次选择保存新记忆时,都会*从头开始*重新生成 profile schema。\n\n这是低效的,如果 schema 包含大量信息需要每次重新生成,可能会浪费 model tokens。\n\n更糟糕的是,当从头开始重新生成 profile 时,我们可能会丢失信息。\n\n解决这些问题正是 [TrustCall](https://github.com/hinthornw/trustcall) 的动机!\n\n这是一个由 LangChain 团队的 [Will Fu-Hinthorn](https://github.com/hinthornw) 开发的用于更新 JSON schemas 的开源库。\n\n它的动机正是在处理记忆时遇到的这些挑战。\n\n让我们首先展示在这个 [messages](https://python.langchain.com/docs/concepts/messages/) 列表上使用 TrustCall 进行提取的简单用法。"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation\n",
    "conversation = [HumanMessage(content=\"Hi, I'm Lance.\"), \n",
    "                AIMessage(content=\"Nice to meet you, Lance.\"), \n",
    "                HumanMessage(content=\"I really like biking around San Francisco.\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "我们使用 `create_extractor`,传入 model 以及我们的 schema 作为 [tool](https://python.langchain.com/docs/concepts/tools/)。\n\n使用 TrustCall,可以以各种方式提供 schema。\n\n例如,我们可以传递 JSON 对象/Python 字典或 Pydantic model。\n\n在底层,TrustCall 使用 [tool calling](https://python.langchain.com/docs/concepts/tool_calling/) 从输入的 [messages](https://python.langchain.com/docs/concepts/messages/) 列表中生成 [structured output](https://python.langchain.com/docs/concepts/structured_outputs/)。\n\n要强制 Trustcall 生成 [structured output](https://python.langchain.com/docs/concepts/structured_outputs/),我们可以在 `tool_choice` 参数中包含 schema 名称。\n\n我们可以使用上面的对话调用 extractor。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from trustcall import create_extractor\n\n# ================== 定义 Schema ==================\n\n# Python 知识点:\n# - 这次使用 Pydantic BaseModel 而不是 TypedDict\n# - Field: Pydantic 字段,可以添加描述、验证规则等\n# - description: 帮助 LLM 理解字段含义,提高提取准确性\n\nclass UserProfile(BaseModel):\n    \"\"\"用户 profile schema - 使用 Pydantic 定义\"\"\"\n    user_name: str = Field(description=\"The user's preferred name\")\n    interests: List[str] = Field(description=\"A list of the user's interests\")\n\n# 初始化 model\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n\n# ================== 创建 Trustcall Extractor ==================\n\n# Trustcall 知识点:\n# - create_extractor: Trustcall 的核心函数\n# - 作用: 创建一个可以提取和更新结构化数据的 extractor\n# - 底层: 使用 tool calling 实现结构化输出\n\n# 参数说明:\n# - model: 要使用的 LLM\n# - tools: Schema 列表,可以是 Pydantic/TypedDict/JSON\n# - tool_choice: 强制使用特定 tool,确保生成结构化输出\n\ntrustcall_extractor = create_extractor(\n    model,\n    tools=[UserProfile],  # 传入 Pydantic model\n    tool_choice=\"UserProfile\"  # 强制使用 UserProfile tool\n)\n\n# ================== 准备系统指令 ==================\n\n# 系统消息 - 指导 LLM 从对话中提取 profile\nsystem_msg = \"Extract the user profile from the following conversation\"\n\n# ================== 调用 Extractor ==================\n\n# Trustcall 工作流程:\n# 1. 接收 messages 列表(SystemMessage + 对话历史)\n# 2. 使用 tool calling 生成 tool call\n# 3. 解析 tool call 参数为 UserProfile 实例\n# 4. 返回包含多个字段的结果字典\n\n# 调用 extractor\n# - 输入: {\"messages\": [SystemMessage, ...对话消息]}\n# - 输出: 包含 messages, responses, response_metadata 的字典\nresult = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=system_msg)]+conversation})"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "当我们调用 extractor 时,我们得到几样东西:\n\n* `messages`: 包含 tool calls 的 `AIMessages` 列表。\n* `responses`: 与我们 schema 匹配的解析后的 tool calls 结果。\n* `response_metadata`: 适用于更新现有 tool calls。它表示哪些 responses 对应于哪些现有对象。"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  UserProfile (call_spGGUsoaUFXU7oOrUNCASzfL)\n",
      " Call ID: call_spGGUsoaUFXU7oOrUNCASzfL\n",
      "  Args:\n",
      "    user_name: Lance\n",
      "    interests: ['biking around San Francisco']\n"
     ]
    }
   ],
   "source": [
    "for m in result[\"messages\"]: \n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UserProfile(user_name='Lance', interests=['biking around San Francisco'])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = result[\"responses\"]\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_name': 'Lance', 'interests': ['biking around San Francisco']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema[0].model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'call_spGGUsoaUFXU7oOrUNCASzfL'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"response_metadata\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "让我们看看如何使用它来*更新* profile。\n\n对于更新,TrustCall 接受一组 messages 以及现有的 schema。\n\n核心思想是它提示 model 生成 [JSON Patch](https://jsonpatch.com/) 来仅更新 schema 的相关部分。\n\n这比天真地覆盖整个 schema 更不容易出错。\n\n它也更高效,因为 model 只需要生成已更改的 schema 部分。\n\n我们可以将现有的 schema 保存为 dict。\n\n我们可以使用 `model_dump()` 将 Pydantic model 实例序列化为 dict。\n\n我们将它连同 schema 名称 `UserProfile` 传递给 `\"existing\"` 参数。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================== 更新对话 ==================\n\n# 新增两条消息,用于测试 Trustcall 的更新功能\nupdated_conversation = [\n    HumanMessage(content=\"Hi, I'm Lance.\"), \n    AIMessage(content=\"Nice to meet you, Lance.\"), \n    HumanMessage(content=\"I really like biking around San Francisco.\"),\n    AIMessage(content=\"San Francisco is a great city! Where do you go after biking?\"),\n    HumanMessage(content=\"I really like to go to a bakery after biking.\"),  # 新信息\n]\n\n# ================== 更新系统指令 ==================\n\n# 关键变化: 从 \"Extract\" 改为 \"Update\"\n# - 指示 LLM 这是更新操作,不是从头创建\n# - LLM 会生成 JSON Patch 来更新现有 schema\nsystem_msg = f\"\"\"Update the memory (JSON doc) to incorporate new information from the following conversation\"\"\"\n\n# ================== 使用 Trustcall 更新 Profile ==================\n\n# Trustcall 更新机制:\n# - existing 参数: 传入现有 schema 的字典表示\n# - 格式: {tool_name: schema_dict}\n# - Trustcall 会:\n#   1. 分析现有 schema 和新对话\n#   2. 生成 JSON Patch 表示变化\n#   3. 应用 patch 到现有 schema\n#   4. 返回更新后的 schema\n\n# Python 知识点:\n# - schema[0]: result[\"responses\"] 是列表,取第一个元素\n# - model_dump(): Pydantic 方法,将 model 实例转换为字典\n\n# 调用 extractor 进行更新\nresult = trustcall_extractor.invoke(\n    {\"messages\": [SystemMessage(content=system_msg)]+updated_conversation},  # 新对话\n    {\"existing\": {\"UserProfile\": schema[0].model_dump()}}  # 现有 schema\n)\n\n# Trustcall 的优势:\n# - 只更新变化的部分,不重新生成整个 schema\n# - 更高效,节省 tokens\n# - 更不容易丢失信息\n# - 使用 JSON Patch 标准,更可靠"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  UserProfile (call_WeZl0ACfQStxblim0ps8LNKT)\n",
      " Call ID: call_WeZl0ACfQStxblim0ps8LNKT\n",
      "  Args:\n",
      "    user_name: Lance\n",
      "    interests: ['biking', 'visiting bakeries']\n"
     ]
    }
   ],
   "source": [
    "for m in result[\"messages\"]: \n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'call_WeZl0ACfQStxblim0ps8LNKT'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"response_metadata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_name': 'Lance', 'interests': ['biking', 'visiting bakeries']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_schema = result[\"responses\"][0]\n",
    "updated_schema.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "LangSmith trace:\n\nhttps://smith.langchain.com/public/229eae22-1edb-44c6-93e6-489124a43968/r\n\n现在,让我们也在之前看到的[复杂 schema](https://github.com/hinthornw/trustcall?tab=readme-ov-file#complex-schema) 上测试 Trustcall。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================== 测试复杂 Schema ==================\n\n# Trustcall 知识点:\n# - Trustcall 专门设计用于处理复杂 schema\n# - 使用迭代策略和 JSON Patch,比 with_structured_output 更可靠\n\n# 创建 extractor for 复杂 schema\nbound = create_extractor(\n    model,\n    tools=[TelegramAndTrustFallPreferences],  # 嵌套很深的 Pydantic model\n    tool_choice=\"TelegramAndTrustFallPreferences\",  # 强制使用此 tool\n)\n\n# ================== 准备测试对话 ==================\n\n# 这是一个关于电报和 trust fall 偏好的复杂对话\nconversation = \"\"\"Operator: How may I assist with your telegram, sir?\nCustomer: I need to send a message about our trust fall exercise.\nOperator: Certainly. Morse code or standard encoding?\nCustomer: Morse, please. I love using a straight key.\nOperator: Excellent. What's your message?\nCustomer: Tell him I'm ready for a higher fall, and I prefer the diamond formation for catching.\nOperator: Done. Shall I use our \"Daredevil\" paper for this daring message?\nCustomer: Perfect! Send it by your fastest carrier pigeon.\nOperator: It'll be there within the hour, sir.\"\"\"\n\n# ================== 调用 Trustcall 提取 ==================\n\n# Trustcall 如何处理复杂 schema:\n# 1. 将复杂 schema 分解为多个简单的子任务\n# 2. 迭代处理每个子 schema\n# 3. 使用 JSON Patch 合并结果\n# 4. 验证最终结果是否符合完整 schema\n\n# 调用 extractor\nresult = bound.invoke(\n    f\"\"\"Extract the preferences from the following conversation:\n<convo>\n{conversation}\n</convo>\"\"\"\n)\n\n# 提取并显示结果\n# - result[\"responses\"]: 包含解析后的 Pydantic model 实例列表\n# - result[\"responses\"][0]: 第一个(也是唯一的)提取结果\nresult[\"responses\"][0]\n\n# 为什么 Trustcall 成功而 with_structured_output 失败:\n# - with_structured_output: 一次性生成整个 schema,容易出错\n# - Trustcall: 分步处理,更容易处理复杂嵌套结构\n# - Trustcall: 使用多轮交互和验证,确保完整性"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Trace: \n\nhttps://smith.langchain.com/public/5cd23009-3e05-4b00-99f0-c66ee3edd06e/r\n\n要获取更多示例,你可以在[这里](https://www.youtube.com/watch?v=-H4s0jQi-QY)查看概述视频。"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 使用 profile schema 更新的 Chatbot\n\n现在,让我们将 Trustcall 引入我们的 chatbot 来*创建和更新*记忆 profile。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from IPython.display import Image, display\n\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langgraph.graph import StateGraph, MessagesState, START, END\nfrom langchain_core.runnables.config import RunnableConfig\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.store.base import BaseStore\n\n# ================== 初始化 Model ==================\n\n# 初始化 LLM\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n\n# ================== 定义 Schema ==================\n\n# 扩展的 UserProfile schema\n# 新增 user_location 字段,展示如何处理更复杂的 profile\nclass UserProfile(BaseModel):\n    \"\"\"完整的用户 profile schema\"\"\"\n    user_name: str = Field(description=\"The user's preferred name\")\n    user_location: str = Field(description=\"The user's location\")\n    interests: list = Field(description=\"A list of the user's interests\")\n\n# ================== 创建 Trustcall Extractor ==================\n\n# Trustcall 知识点:\n# - 这个 extractor 将用于 write_memory 节点\n# - 相比 with_structured_output,Trustcall 提供:\n#   1. 更好的复杂 schema 支持\n#   2. 增量更新能力(使用 JSON Patch)\n#   3. 更少的信息丢失\n#   4. 更高的 token 效率\n\ntrustcall_extractor = create_extractor(\n    model,\n    tools=[UserProfile],\n    tool_choice=\"UserProfile\",  # 强制使用 UserProfile,确保结构化输出\n)\n\n# ================== 系统提示词 ==================\n\n# Chatbot 主对话提示\nMODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful assistant with memory that provides information about the user. \nIf you have memory for this user, use it to personalize your responses.\nHere is the memory (it may be empty): {memory}\"\"\"\n\n# Trustcall 提取提示\n# 关键词 \"Create or update\" 告诉 Trustcall 这是创建/更新操作\nTRUSTCALL_INSTRUCTION = \"\"\"Create or update the memory (JSON doc) to incorporate information from the following conversation:\"\"\"\n\n# ================== call_model 节点 ==================\n\ndef call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n    \"\"\"\n    主对话节点 - 从 store 加载记忆并生成个性化响应\n    \n    与 cell-18 的 call_model 基本相同,但使用了扩展的 UserProfile schema\n    \n    工作流程:\n        1. 从 config 获取 user_id\n        2. 从 store 检索该用户的记忆\n        3. 格式化记忆(现在包含 location)\n        4. 注入系统提示并生成响应\n    \"\"\"\n    \n    # 获取 user_id\n    user_id = config[\"configurable\"][\"user_id\"]\n\n    # 从 store 检索记忆\n    namespace = (\"memory\", user_id)\n    existing_memory = store.get(namespace, \"user_memory\")\n\n    # 格式化记忆 - 现在包含三个字段\n    if existing_memory and existing_memory.value:\n        memory_dict = existing_memory.value\n        formatted_memory = (\n            f\"Name: {memory_dict.get('user_name', 'Unknown')}\\n\"\n            f\"Location: {memory_dict.get('user_location', 'Unknown')}\\n\"  # 新增字段\n            f\"Interests: {', '.join(memory_dict.get('interests', []))}\"      \n        )\n    else:\n        formatted_memory = None\n\n    # 格式化系统消息\n    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=formatted_memory)\n\n    # 生成响应\n    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n\n    return {\"messages\": response}\n\n# ================== write_memory 节点 ==================\n\ndef write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n    \"\"\"\n    记忆写入节点 - 使用 Trustcall 创建和更新记忆 profile\n    \n    关键改进 (相比 cell-18):\n    - 使用 Trustcall 而不是 with_structured_output\n    - 支持增量更新,避免从头重新生成\n    - 更不容易丢失信息\n    - 更高效的 token 使用\n    \n    工作流程:\n        1. 从 config 获取 user_id\n        2. 从 store 检索现有记忆\n        3. 将现有记忆转换为 Trustcall 可接受的格式\n        4. 调用 Trustcall extractor 进行更新\n        5. 保存更新后的 profile\n    \n    Trustcall vs with_structured_output:\n    - with_structured_output: 每次从头生成整个 schema\n    - Trustcall: 生成 JSON Patch,只更新变化的部分\n    \"\"\"\n    \n    # 获取 user_id\n    user_id = config[\"configurable\"][\"user_id\"]\n\n    # 检索现有记忆\n    namespace = (\"memory\", user_id)\n    existing_memory = store.get(namespace, \"user_memory\")\n        \n    # ========== 准备 existing 参数 ==========\n    # Trustcall 知识点:\n    # - existing 参数告诉 Trustcall 这是更新操作\n    # - 格式: {tool_name: schema_dict}\n    # - 如果 existing_memory 存在,使用其值\n    # - 如果不存在,设置为 None,Trustcall 会创建新 profile\n    \n    # Python 知识点:\n    # - 三元表达式: value_if_true if condition else value_if_false\n    # - {\"UserProfile\": dict} 创建嵌套字典\n    existing_profile = {\"UserProfile\": existing_memory.value} if existing_memory else None\n    \n    # ========== 调用 Trustcall Extractor ==========\n    # 工作流程:\n    # 1. Trustcall 接收对话历史和现有 profile\n    # 2. 如果是更新:\n    #    a. 分析对话中的新信息\n    #    b. 生成 JSON Patch 表示变化\n    #    c. 应用 patch 到现有 profile\n    # 3. 如果是创建:\n    #    a. 从对话中提取所有信息\n    #    b. 生成完整的新 profile\n    # 4. 验证结果是否符合 UserProfile schema\n    \n    result = trustcall_extractor.invoke(\n        {\"messages\": [SystemMessage(content=TRUSTCALL_INSTRUCTION)]+state[\"messages\"]}, \n        {\"existing\": existing_profile}  # 传入现有 profile\n    )\n    \n    # ========== 提取和保存 Profile ==========\n    # result[\"responses\"]: Trustcall 返回的 Pydantic model 实例列表\n    # [0]: 取第一个(通常是唯一的)结果\n    # .model_dump(): 将 Pydantic model 转换为字典\n    updated_profile = result[\"responses\"][0].model_dump()\n\n    # 保存到 store\n    # Python 知识点: 这是覆盖式保存,但实际内容是增量更新的\n    key = \"user_memory\"\n    store.put(namespace, key, updated_profile)\n\n# ================== 构建 Graph ==================\n\n# 定义图 - 与 cell-18 相同的结构\nbuilder = StateGraph(MessagesState)\n\n# 添加节点\nbuilder.add_node(\"call_model\", call_model)\nbuilder.add_node(\"write_memory\", write_memory)\n\n# 添加边 - 简单的线性流程\n# START -> call_model -> write_memory -> END\nbuilder.add_edge(START, \"call_model\")\nbuilder.add_edge(\"call_model\", \"write_memory\")\nbuilder.add_edge(\"write_memory\", END)\n\n# ================== 初始化双记忆系统 ==================\n\n# 长期记忆 (跨线程) - Store\n# - 保存用户 profile\n# - 使用 Trustcall 管理\n# - 增量更新,避免信息丢失\nacross_thread_memory = InMemoryStore()\n\n# 短期记忆 (线程内) - Checkpointer  \n# - 保存对话历史\n# - 每个线程独立\nwithin_thread_memory = MemorySaver()\n\n# ================== 编译 Graph ==================\n\n# LangGraph 知识点:\n# - checkpointer + store: 双记忆架构\n# - checkpointer: 管理对话历史 (MessagesState)\n# - store: 管理长期记忆 (用户 profile)\n# - 两者协同工作,提供完整的记忆能力\n\ngraph = builder.compile(\n    checkpointer=within_thread_memory, \n    store=across_thread_memory\n)\n\n# 可视化图结构\ndisplay(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n\n# ================== 总结: Trustcall Chatbot 的优势 ==================\n#\n# 1. 增量更新:\n#    - 只更新变化的字段,不重新生成整个 profile\n#    - 减少信息丢失的风险\n#\n# 2. Token 效率:\n#    - 不需要每次生成完整 profile\n#    - 只生成 JSON Patch,节省 tokens\n#\n# 3. 复杂 Schema 支持:\n#    - 比 with_structured_output 更可靠\n#    - 可以处理深度嵌套的 schema\n#\n# 4. 灵活性:\n#    - 同一个 extractor 可以用于创建和更新\n#    - 通过 existing 参数控制行为\n#\n# 5. 标准化:\n#    - 使用 JSON Patch 标准\n#    - 更容易调试和理解变化"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, my name is Lance\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Lance! It's nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Lance\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I like to bike around San Francisco\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That sounds like a great way to explore the city! San Francisco has some beautiful routes and views. Do you have any favorite trails or spots you like to visit while biking?\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I like to bike around San Francisco\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': {'user_name': 'Lance',\n",
       "  'user_location': 'San Francisco',\n",
       "  'interests': ['biking']},\n",
       " 'key': 'user_memory',\n",
       " 'namespace': ['memory', '1'],\n",
       " 'created_at': '2024-11-04T23:51:17.662428+00:00',\n",
       " 'updated_at': '2024-11-04T23:51:41.697652+00:00'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace = (\"memory\", user_id)\n",
    "existing_memory = across_thread_memory.get(namespace, \"user_memory\")\n",
    "existing_memory.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_name': 'Lance',\n",
       " 'user_location': 'San Francisco',\n",
       " 'interests': ['biking']}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The user profile saved as a JSON object\n",
    "existing_memory.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I also enjoy going to bakeries\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Biking and visiting bakeries sounds like a delightful combination! San Francisco has some fantastic bakeries. Do you have any favorites, or are you looking for new recommendations to try out?\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I also enjoy going to bakeries\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "在新线程中继续对话。"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What bakeries do you recommend for me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Since you're in San Francisco and enjoy going to bakeries, here are a few recommendations you might like:\n",
      "\n",
      "1. **Tartine Bakery** - Known for its delicious bread and pastries, it's a must-visit for any bakery enthusiast.\n",
      "2. **B. Patisserie** - Offers a delightful selection of French pastries, including their famous kouign-amann.\n",
      "3. **Arsicault Bakery** - Renowned for its croissants, which have been praised as some of the best in the country.\n",
      "4. **Craftsman and Wolves** - Known for their inventive pastries and the \"Rebel Within,\" a savory muffin with a soft-cooked egg inside.\n",
      "5. **Mr. Holmes Bakehouse** - Famous for their cruffins and other creative pastries.\n",
      "\n",
      "These spots should offer a great variety of treats for you to enjoy. Happy bakery hopping!\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"What bakeries do you recommend for me?\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Trace:\n\nhttps://smith.langchain.com/public/f45bdaf0-6963-4c19-8ec9-f4b7fe0f68ad/r\n\n## Studio\n\n![Screenshot 2024-10-30 at 11.26.31 AM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6732d0437060f1754ea79908_Screenshot%202024-11-11%20at%207.48.53%E2%80%AFPM.png)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}