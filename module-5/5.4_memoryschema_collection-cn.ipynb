{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 使用 Collection Schema 的 Chatbot\n\n## 回顾\n\n我们扩展了我们的 chatbot,将语义记忆保存到单一的 [用户 profile](https://langchain-ai.github.io/langgraph/concepts/memory/#profile) 中。\n\n我们还介绍了一个库,[Trustcall](https://github.com/hinthornw/trustcall),用于使用新信息更新这个 schema。\n\n## 目标\n\n有时我们希望将记忆保存到 [collection](https://docs.google.com/presentation/d/181mvjlgsnxudQI6S3ritg9sooNyu4AcLLFH1UK0kIuk/edit#slide=id.g30eb3c8cf10_0_200) 而不是单一的 profile。\n\n在这里,我们将更新我们的 chatbot 来[将记忆保存到 collection](https://langchain-ai.github.io/langgraph/concepts/memory/#collection)。\n\n我们还将展示如何使用 [Trustcall](https://github.com/hinthornw/trustcall) 来更新这个 collection。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain_openai langgraph trustcall langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, getpass\n\ndef _set_env(var: str):\n    \"\"\"\n    辅助函数 - 设置环境变量\n    \n    Python 知识点:\n    - os.environ: Python 的环境变量字典\n    - getpass.getpass(): 安全地提示用户输入密码\n    \n    工作流程:\n        1. 检查环境变量是否已设置\n        2. 如果未设置,提示用户输入\n        3. 设置到当前进程的环境变量中\n    \"\"\"\n    # 检查环境变量是否已在 OS 环境中设置\n    env_value = os.environ.get(var)\n    if not env_value:\n        # 如果未设置,提示用户输入\n        env_value = getpass.getpass(f\"{var}: \")\n    \n    # 为当前进程设置环境变量\n    os.environ[var] = env_value\n\n# ================== LangSmith 配置 ==================\n# LangSmith: LangChain 的追踪和调试平台\n\n# 设置 LangSmith API 密钥\n_set_env(\"LANGSMITH_API_KEY\")\n\n# 启用 LangSmith 追踪功能\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\n\n# 设置 LangSmith 项目名称\nos.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 定义 collection schema\n\n与其将用户信息存储在固定的 profile 结构中,我们将创建一个灵活的 collection schema 来存储关于用户互动的记忆。\n\n每个记忆将作为单独的条目存储,只有一个 `content` 字段用于存储我们想要记住的主要信息。\n\n这种方法允许我们构建一个开放式的记忆 collection,可以随着我们对用户了解的增加而增长和变化。\n\n我们可以将 collection schema 定义为 [Pydantic](https://docs.pydantic.dev/latest/) 对象。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pydantic import BaseModel, Field\n\n# ================== 定义 Collection Schema ==================\n\n# Collection vs Profile:\n# - Profile: 单一的、结构化的用户档案,包含固定字段(name, location, interests)\n# - Collection: 多个独立的记忆条目,每个条目是一个独立的 Memory 对象\n# \n# Collection 的优势:\n# - 灵活性: 可以添加任意数量的记忆,无需预定义结构\n# - 独立性: 每个记忆可以独立更新或删除\n# - 扩展性: 适合存储开放式的、不断增长的信息\n\nclass Memory(BaseModel):\n    \"\"\"\n    单个记忆条目 - Collection 中的基本单元\n    \n    Pydantic 知识点:\n    - BaseModel: Pydantic 的基类,提供数据验证和序列化\n    - Field: 定义字段的元数据,如描述、验证规则等\n    \n    为什么只有一个字段:\n    - 简单性: 每个记忆只包含核心内容\n    - 灵活性: content 可以包含各种类型的信息\n    - 可扩展: 未来可以添加更多字段(timestamp, importance, etc.)\n    \"\"\"\n    content: str = Field(\n        description=\"The main content of the memory. For example: User expressed interest in learning about French.\"\n    )\n\nclass MemoryCollection(BaseModel):\n    \"\"\"\n    记忆集合 - 包含多个 Memory 的列表\n    \n    用途:\n    - 用于 with_structured_output 一次性提取多个记忆\n    - 不直接保存到 Store,而是将每个 Memory 单独保存\n    \"\"\"\n    memories: list[Memory] = Field(\n        description=\"A list of memories about the user.\"\n    )"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "我们可以使用 LangChain 的 [chat model](https://python.langchain.com/docs/concepts/chat_models/) 接口的 [`with_structured_output`](https://python.langchain.com/docs/concepts/structured_outputs/#recommended-usage) 方法来强制执行结构化输出。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\n\n# ================== 使用 with_structured_output 提取记忆 ==================\n\n# 初始化 model\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n\n# 将 MemoryCollection schema 绑定到 model\n# LangChain 知识点:\n# - with_structured_output: 强制 LLM 输出符合指定 schema 的数据\n# - 返回的是 MemoryCollection 实例,包含 memories 列表\nmodel_with_structure = model.with_structured_output(MemoryCollection)\n\n# ================== 调用 Model 提取记忆 ==================\n# 工作流程:\n# 1. LLM 分析输入消息\n# 2. 识别多个独立的信息点\n# 3. 为每个信息点创建一个 Memory 对象\n# 4. 返回 MemoryCollection 包含所有 Memory\n\n# 调用 model 从用户消息中提取记忆\n# 输入: \"My name is Lance. I like to bike.\"\n# 预期输出: MemoryCollection 包含两个 Memory:\n#   1. Memory(content=\"User's name is Lance.\")\n#   2. Memory(content='Lance likes to bike.')\nmemory_collection = model_with_structure.invoke([HumanMessage(\"My name is Lance. I like to bike.\")])\n\n# 显示提取的记忆列表\nmemory_collection.memories"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "我们可以使用 `model_dump()` 将 Pydantic model 实例序列化为 Python 字典。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================== 序列化 Pydantic Model ==================\n\n# Python 知识点:\n# - model_dump(): Pydantic 方法,将 model 实例转换为 Python 字典\n# - 这是保存到 Store 前的必要步骤,因为 Store 只接受字典\n\n# 将第一个 Memory 转换为字典\n# 输入: Memory(content=\"User's name is Lance.\")\n# 输出: {'content': \"User's name is Lance.\"}\nmemory_collection.memories[0].model_dump()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "将每个记忆的字典表示保存到 store。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import uuid\nfrom langgraph.store.memory import InMemoryStore\n\n# ================== 初始化 Store ==================\n\n# 初始化内存 store\nin_memory_store = InMemoryStore()\n\n# ================== 定义 Namespace ==================\n# Collection 存储策略:\n# - Namespace: (user_id, \"memories\") \n# - 注意: 使用 \"memories\" 而不是 \"memory\",表示这是多个记忆的集合\n# - 每个用户有一个独立的 memories namespace\n\nuser_id = \"1\"\nnamespace_for_memory = (user_id, \"memories\")\n\n# ================== 保存记忆到 Store ==================\n# Collection 保存模式:\n# - 每个 Memory 作为独立的条目保存\n# - Key: 使用 UUID 生成唯一标识符\n# - Value: Memory 的字典表示\n# \n# 这与 Profile 不同:\n# - Profile: 一个 key (\"user_memory\") 保存整个 profile\n# - Collection: 多个 key (UUID) 每个保存一个 Memory\n\n# Python 知识点:\n# - uuid.uuid4(): 生成随机的 UUID (Universally Unique Identifier)\n# - str(uuid): 将 UUID 对象转换为字符串\n\n# 保存第一个记忆\nkey = str(uuid.uuid4())  # 生成唯一 key,例如: \"e1c4e5ab-ab0f-4cbb-822d-f29240a983af\"\nvalue = memory_collection.memories[0].model_dump()  # {'content': \"User's name is Lance.\"}\nin_memory_store.put(namespace_for_memory, key, value)\n\n# 保存第二个记忆\nkey = str(uuid.uuid4())  # 生成另一个唯一 key\nvalue = memory_collection.memories[1].model_dump()  # {'content': 'Lance likes to bike.'}\nin_memory_store.put(namespace_for_memory, key, value)\n\n# Collection 保存的优势:\n# - 每个记忆可以独立更新或删除\n# - 可以轻松添加新记忆而不影响现有记忆\n# - 支持按需检索特定记忆"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "在 store 中搜索记忆。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================== 搜索 Collection 中的记忆 ==================\n\n# LangGraph Store 知识点:\n# - search(namespace): 检索指定 namespace 下的所有记忆\n# - 返回 Item 对象列表,每个包含 key, value, namespace, created_at, updated_at\n\n# 搜索并打印所有记忆\n# 输出格式:\n# - value: 记忆内容 {'content': '...'}\n# - key: UUID 标识符\n# - namespace: ['1', 'memories']\n# - created_at/updated_at: 时间戳\nfor m in in_memory_store.search(namespace_for_memory):\n    print(m.dict())\n\n# Collection 搜索的特点:\n# - 返回多个独立的 Item 对象\n# - 每个 Item 有唯一的 key\n# - 可以按时间排序或过滤"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 更新 collection schema\n\n我们在上一课中讨论了更新 profile schema 的挑战。\n\n同样的问题也适用于 collections!\n\n我们希望能够用新记忆更新 collection,以及更新 collection 中的现有记忆。\n\n现在我们将展示 [Trustcall](https://github.com/hinthornw/trustcall) 也可以用于更新 collection。\n\n这使得既可以添加新记忆,也可以[更新 collection 中的现有记忆](https://github.com/hinthornw/trustcall?tab=readme-ov-file#simultanous-updates--insertions)。\n\n让我们使用 Trustcall 定义一个新的 extractor。\n\n和之前一样,我们为每个记忆提供 schema,即 `Memory`。\n\n但是,我们可以提供 `enable_inserts=True` 来允许 extractor 向 collection 插入新记忆。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from trustcall import create_extractor\n\n# ================== 创建 Trustcall Extractor for Collection ==================\n\n# Trustcall 知识点:\n# - create_extractor: 创建可以提取和更新结构化数据的 extractor\n# - 与 Profile 不同,Collection 需要特殊配置\n\n# 创建 extractor\ntrustcall_extractor = create_extractor(\n    model,\n    tools=[Memory],  # 注意: 传入 Memory 而不是 MemoryCollection\n    tool_choice=\"Memory\",  # 强制使用 Memory tool\n    \n    # ========== 关键参数: enable_inserts ==========\n    # enable_inserts=True: 允许 extractor 插入新记忆到 collection\n    # \n    # 工作模式:\n    # - 如果 enable_inserts=False: 只能更新现有记忆\n    # - 如果 enable_inserts=True: 可以同时更新现有记忆和插入新记忆\n    # \n    # Collection 的核心特性:\n    # - 并行更新和插入: Trustcall 可以同时进行多个操作\n    # - 自动识别: LLM 决定是更新现有记忆还是创建新记忆\n    enable_inserts=True,\n)\n\n# 与 Profile Extractor 的区别:\n# 1. Profile: 不需要 enable_inserts,因为总是更新同一个对象\n# 2. Collection: 需要 enable_inserts,因为要支持添加新记忆"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n\n# ================== 准备提取指令 ==================\n\n# 系统指令 - 告诉 LLM 从对话中提取记忆\ninstruction = \"\"\"Extract memories from the following conversation:\"\"\"\n\n# ================== 准备对话历史 ==================\n\n# 对话历史包含三条消息\nconversation = [\n    HumanMessage(content=\"Hi, I'm Lance.\"),  # 包含名字信息\n    AIMessage(content=\"Nice to meet you, Lance.\"),  # AI 响应\n    HumanMessage(content=\"This morning I had a nice bike ride in San Francisco.\")  # 包含活动信息\n]\n\n# ================== 调用 Trustcall Extractor ==================\n\n# Trustcall 工作流程:\n# 1. 接收 SystemMessage (指令) + 对话历史\n# 2. 分析对话,识别需要记住的信息\n# 3. 使用 tool calling 生成 Memory tool calls\n# 4. 解析 tool calls 为 Memory 实例\n# 5. 返回结果字典\n\n# 调用 extractor\n# 输入格式: {\"messages\": [SystemMessage, ...对话消息]}\nresult = trustcall_extractor.invoke({\n    \"messages\": [SystemMessage(content=instruction)] + conversation\n})\n\n# 预期行为:\n# - LLM 会识别出信息: \"Lance had a nice bike ride in San Francisco this morning.\"\n# - 合并多个信息点为一个连贯的记忆\n# - 返回包含 messages, responses, response_metadata 的字典"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================== 查看 Tool Calls (Messages) ==================\n\n# Trustcall 返回结果的第一部分: messages\n# - 包含 AIMessage 和其中的 tool calls\n# - Tool calls 是 LLM 生成的结构化函数调用\n\n# 打印所有 messages\n# 输出格式:\n# - Tool Calls: Memory (call_id)\n# - Args: content=\"记忆内容\"\nfor m in result[\"messages\"]:\n    m.pretty_print()\n\n# LangChain 知识点:\n# - Tool calling: LLM 生成的函数调用,包含 tool 名称、call_id 和参数\n# - AIMessage: 包含 tool_calls 字段,是一个 tool call 列表"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================== 查看 Responses (解析后的记忆) ==================\n\n# Trustcall 返回结果的第二部分: responses\n# - 包含解析后的 Memory 实例列表\n# - 每个 Memory 实例符合我们定义的 schema\n\n# 打印所有 responses\n# 输出: Memory(content='Lance had a nice bike ride in San Francisco this morning.')\nfor m in result[\"responses\"]: \n    print(m)\n\n# Trustcall 知识点:\n# - responses: tool calls 的解析结果\n# - 自动验证和转换为 Pydantic model 实例\n# - 确保数据符合 schema 定义"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================== 查看 Metadata (Tool Call 元数据) ==================\n\n# Trustcall 返回结果的第三部分: response_metadata\n# - 包含每个 response 的元数据\n# - 最重要的是 'id' 字段,对应 tool call 的 call_id\n\n# 打印所有 metadata\n# 输出: {'id': 'call_Pj4kctFlpg9TgcMBfMH33N30'}\nfor m in result[\"response_metadata\"]: \n    print(m)\n\n# Metadata 的用途:\n# - id: 将 response 与对应的 tool call 关联\n# - json_doc_id: (可选) 如果是更新操作,标识被更新的文档\n# - 用于追踪和调试"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================== 准备更新对话 ==================\n\n# 扩展对话 - 添加新的交互\nupdated_conversation = [\n    AIMessage(content=\"That's great, did you do after?\"),  # AI 询问后续\n    HumanMessage(content=\"I went to Tartine and ate a croissant.\"),  # 新信息: 去了面包店\n    AIMessage(content=\"What else is on your mind?\"),  # AI 询问其他\n    HumanMessage(content=\"I was thinking about my Japan, and going back this winter!\"),  # 新信息: 日本计划\n]\n\n# ================== 更新系统指令 ==================\n\n# 关键变化: \"Update existing memories and create new ones\"\n# - 告诉 Trustcall 这是更新操作,不是从头创建\n# - Trustcall 会决定:\n#   1. 哪些信息应该更新现有记忆\n#   2. 哪些信息应该创建新记忆\nsystem_msg = \"\"\"Update existing memories and create new ones based on the following conversation:\"\"\"\n\n# ================== 准备 existing 参数 ==================\n\n# Trustcall Collection 更新的核心: existing 参数\n# - 格式: List[Tuple[id, tool_name, value]]\n# - 每个元组包含三个元素:\n#   1. id: 记忆的标识符 (字符串)\n#   2. tool_name: tool 名称 (\"Memory\")\n#   3. value: 记忆的字典表示\n\n# Python 知识点:\n# - enumerate(list): 返回 (index, item) 元组\n# - str(i): 将索引转换为字符串作为 id\n# - memory.model_dump(): 将 Memory 实例转换为字典\n\ntool_name = \"Memory\"\n\n# 创建 existing_memories 列表\n# 格式: [('0', 'Memory', {'content': '...'})]\nexisting_memories = (\n    [(str(i), tool_name, memory.model_dump()) \n     for i, memory in enumerate(result[\"responses\"])]  # 遍历之前提取的记忆\n    if result[\"responses\"]  # 如果有记忆\n    else None  # 否则为 None\n)\n\n# 输出示例:\n# [('0', 'Memory', {'content': 'Lance had a nice bike ride in San Francisco this morning.'})]\nexisting_memories\n\n# Collection 更新的关键点:\n# - id (这里是 '0'): 用于标识要更新的记忆\n# - Trustcall 会使用这个 id 来决定是更新还是插入"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================== 调用 Trustcall 进行更新和插入 ==================\n\n# Trustcall Collection 更新机制:\n# 1. 分析 updated_conversation 中的新信息\n# 2. 比对 existing_memories 中的现有记忆\n# 3. 决定哪些记忆需要更新,哪些需要插入\n# 4. 生成相应的 tool calls\n# 5. 返回更新后的结果\n\n# 调用 extractor\nresult = trustcall_extractor.invoke({\n    \"messages\": updated_conversation,  # 新的对话内容\n    \"existing\": existing_memories  # 现有记忆列表\n})\n\n# Trustcall 的智能决策:\n# - 如果新信息与现有记忆相关: 更新现有记忆 (标记 json_doc_id)\n# - 如果新信息是独立的: 创建新记忆 (不标记 json_doc_id)\n# \n# 在这个例子中:\n# - \"went to Tartine\" 和 \"Japan trip\" 可能与 \"bike ride\" 相关\n# - Trustcall 会决定是更新现有记忆还是创建新记忆"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================== 查看更新后的 Tool Calls ==================\n\n# 打印所有 messages\n# 预期看到两个 Memory tool calls:\n# 1. 一个更新现有记忆 (包含所有相关信息)\n# 2. 一个创建新记忆 (独立的新信息)\nfor m in result[\"messages\"]:\n    m.pretty_print()\n\n# 输出分析:\n# Tool Call 1:\n#   content: 'Lance had a nice bike ride in San Francisco this morning. \n#             He went to Tartine and ate a croissant. \n#             He was thinking about his trip to Japan and going back this winter!'\n#   - 这是更新操作,合并了多个相关信息\n# \n# Tool Call 2:\n#   content: 'Lance went to Tartine and ate a croissant. \n#             He was thinking about his trip to Japan and going back this winter!'\n#   - 这是插入操作,创建新的独立记忆\n\n# Trustcall 的并行操作:\n# - 使用 parallel tool calling 同时进行更新和插入\n# - 一次 LLM 调用完成多个操作,提高效率"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================== 查看更新后的 Responses ==================\n\n# 打印所有 responses\n# 显示更新后的记忆内容\nfor m in result[\"responses\"]: \n    print(m)\n\n# 输出:\n# 1. Memory(content='Lance had a nice bike ride...')  - 更新后的记忆\n# 2. Memory(content='Lance went to Tartine...')  - 新插入的记忆\n\n# Collection 更新的结果:\n# - 第一个记忆被扩充,包含了更多上下文信息\n# - 第二个记忆是新创建的,包含独立的信息\n# - 两个记忆都符合 Memory schema"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "这告诉我们,我们通过指定 `json_doc_id` 更新了 collection 中的第一个记忆。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================== 查看 Metadata - 识别更新 vs 插入 ==================\n\n# 打印所有 metadata\n# 关键信息: json_doc_id 字段\nfor m in result[\"response_metadata\"]: \n    print(m)\n\n# 输出分析:\n# 1. {'id': 'call_vxks...', 'json_doc_id': '0'}  - 有 json_doc_id\n#    - 这表示更新操作\n#    - json_doc_id='0' 对应 existing_memories 中的第一个记忆\n#    - 使用这个 id 覆盖现有记忆\n# \n# 2. {'id': 'call_Y4S3...'}  - 没有 json_doc_id\n#    - 这表示插入操作\n#    - 需要生成新的 UUID 作为 key\n\n# json_doc_id 的作用:\n# - 有 json_doc_id: 使用它作为 key,覆盖现有记忆\n# - 无 json_doc_id: 生成新 UUID,插入新记忆\n# \n# 这就是 Trustcall 实现同时更新和插入的机制!\n\n# 在 chatbot 中的应用:\n# - write_memory 函数会检查 json_doc_id\n# - 如果存在,使用它作为 key 保存 (更新)\n# - 如果不存在,生成新 UUID 作为 key 保存 (插入)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "LangSmith trace: \n\nhttps://smith.langchain.com/public/ebc1cb01-f021-4794-80c0-c75d6ea90446/r"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 使用 collection schema 更新的 Chatbot\n\n现在,让我们将 Trustcall 引入我们的 chatbot 来创建和更新记忆 collection。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from IPython.display import Image, display\n\nimport uuid\n\nfrom langgraph.graph import StateGraph, MessagesState, START, END\nfrom langgraph.store.memory import InMemoryStore\nfrom langchain_core.messages import merge_message_runs\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_core.runnables.config import RunnableConfig\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.store.base import BaseStore\n\n# ================== 初始化 Model ==================\n\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n\n# ================== 定义 Memory Schema ==================\n\n# Collection Schema: 每个记忆是独立的 Memory 对象\nclass Memory(BaseModel):\n    \"\"\"单个记忆条目 - Collection 中的基本单元\"\"\"\n    content: str = Field(\n        description=\"The main content of the memory. For example: User expressed interest in learning about French.\"\n    )\n\n# ================== 创建 Trustcall Extractor ==================\n\n# Trustcall 知识点:\n# - enable_inserts=True: 允许同时更新现有记忆和插入新记忆\n# - 这是 Collection 管理的核心能力\n\ntrustcall_extractor = create_extractor(\n    model,\n    tools=[Memory],\n    tool_choice=\"Memory\",\n    enable_inserts=True,  # 允许插入新记忆\n)\n\n# ================== 系统提示词 ==================\n\n# Chatbot 主对话提示\nMODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful chatbot. You are designed to be a companion to a user. \n\nYou have a long term memory which keeps track of information you learn about the user over time.\n\nCurrent Memory (may include updated memories from this conversation): \n\n{memory}\"\"\"\n\n# Trustcall 提取提示\n# 关键指令: \"Use parallel tool calling to handle updates and insertions simultaneously\"\n# - 告诉 LLM 使用并行 tool calling\n# - 同时处理更新和插入操作\nTRUSTCALL_INSTRUCTION = \"\"\"Reflect on following interaction. \n\nUse the provided tools to retain any necessary memories about the user. \n\nUse parallel tool calling to handle updates and insertions simultaneously:\"\"\"\n\n# ================== call_model 节点 ==================\n\ndef call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n    \"\"\"\n    主对话节点 - 从 store 加载记忆集合并生成个性化响应\n    \n    与 Profile Chatbot 的区别:\n    - Profile: 加载单个 profile,格式化为结构化信息\n    - Collection: 加载多个记忆,格式化为列表\n    \n    工作流程:\n        1. 从 config 获取 user_id\n        2. 从 store 搜索该用户的所有记忆\n        3. 将记忆列表格式化为文本\n        4. 注入系统提示并生成响应\n    \"\"\"\n    \n    # 获取 user_id\n    user_id = config[\"configurable\"][\"user_id\"]\n\n    # ========== 从 Store 检索所有记忆 ==========\n    # Collection 知识点:\n    # - search(): 返回所有匹配 namespace 的 Item 对象\n    # - 不是 get(),因为 Collection 有多个记忆\n    namespace = (\"memories\", user_id)  # 注意: 使用 \"memories\"\n    memories = store.search(namespace)\n\n    # ========== 格式化记忆列表 ==========\n    # Python 知识点:\n    # - join(): 将列表元素连接为字符串\n    # - f-string: 格式化每个记忆为 \"- content\"\n    # - mem.value['content']: 从 Item 对象获取记忆内容\n    \n    # 格式化为带有项目符号的列表\n    # 输出示例:\n    # - User's name is Lance.\n    # - User likes to bike around San Francisco.\n    # - User enjoys going to bakeries.\n    info = \"\\n\".join(f\"- {mem.value['content']}\" for mem in memories)\n    \n    # 注入记忆到系统提示\n    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=info)\n\n    # 生成响应\n    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n\n    return {\"messages\": response}\n\n# ================== write_memory 节点 ==================\n\ndef write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n    \"\"\"\n    记忆写入节点 - 使用 Trustcall 更新和插入记忆 collection\n    \n    核心功能:\n    - 同时支持更新现有记忆和插入新记忆\n    - 使用 json_doc_id 区分更新和插入\n    - 高效管理记忆集合\n    \n    工作流程:\n        1. 从 config 获取 user_id\n        2. 从 store 检索现有记忆列表\n        3. 准备 existing 参数 (key, tool_name, value 元组)\n        4. 调用 Trustcall extractor 进行更新/插入\n        5. 遍历结果,根据 json_doc_id 决定使用的 key\n        6. 保存更新或新记忆到 store\n    \"\"\"\n    \n    # 获取 user_id\n    user_id = config[\"configurable\"][\"user_id\"]\n\n    # 定义 namespace\n    namespace = (\"memories\", user_id)\n\n    # ========== 检索现有记忆 ==========\n    existing_items = store.search(namespace)\n\n    # ========== 准备 existing 参数 ==========\n    # Trustcall Collection 更新的核心格式:\n    # - List[Tuple[key, tool_name, value]]\n    # - key: Store 中的 key (UUID 字符串)\n    # - tool_name: \"Memory\"\n    # - value: 记忆的字典表示\n    \n    tool_name = \"Memory\"\n    existing_memories = (\n        [(existing_item.key, tool_name, existing_item.value)  # 使用 Store 的 key\n         for existing_item in existing_items]\n        if existing_items\n        else None\n    )\n    \n    # 关键区别:\n    # - Profile: existing = {\"UserProfile\": profile_dict}\n    # - Collection: existing = [('uuid1', 'Memory', {...}), ('uuid2', 'Memory', {...})]\n\n    # ========== 合并消息 ==========\n    # LangChain 知识点:\n    # - merge_message_runs(): 合并连续的相同角色消息\n    # - 用于清理对话历史,避免重复\n    updated_messages = list(merge_message_runs(\n        messages=[SystemMessage(content=TRUSTCALL_INSTRUCTION)] + state[\"messages\"]\n    ))\n\n    # ========== 调用 Trustcall Extractor ==========\n    # 工作流程:\n    # 1. Trustcall 分析对话和现有记忆\n    # 2. 决定哪些记忆需要更新 (标记 json_doc_id)\n    # 3. 决定哪些记忆需要插入 (不标记 json_doc_id)\n    # 4. 生成相应的 tool calls\n    # 5. 返回结果\n    result = trustcall_extractor.invoke({\n        \"messages\": updated_messages, \n        \"existing\": existing_memories\n    })\n\n    # ========== 保存记忆到 Store ==========\n    # Python 知识点:\n    # - zip(list1, list2): 并行遍历两个列表\n    # - r: Memory 实例 (response)\n    # - rmeta: 元数据字典 (response_metadata)\n    \n    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n        # ========== 决定使用的 Key ==========\n        # Collection 更新的关键逻辑:\n        # \n        # 1. 检查 rmeta 中是否有 json_doc_id:\n        #    - 有: 这是更新操作,使用 json_doc_id 作为 key\n        #    - 无: 这是插入操作,生成新 UUID 作为 key\n        # \n        # 2. rmeta.get(\"json_doc_id\", default):\n        #    - 如果有 json_doc_id,返回它 (例如: 'uuid1')\n        #    - 如果没有,返回 default (新生成的 UUID)\n        \n        # 保存到 store\n        store.put(\n            namespace,\n            rmeta.get(\"json_doc_id\", str(uuid.uuid4())),  # 更新用原key,插入用新key\n            r.model_dump(mode=\"json\"),  # 转换为字典\n        )\n    \n    # Collection 保存的优势:\n    # - 灵活性: 可以同时更新和插入\n    # - 效率: 一次操作完成多个记忆的管理\n    # - 智能: LLM 决定哪些记忆需要更新,哪些需要创建\n\n# ================== 构建 Graph ==================\n\n# 定义图 - 与 Profile Chatbot 相同的结构\nbuilder = StateGraph(MessagesState)\n\n# 添加节点\nbuilder.add_node(\"call_model\", call_model)\nbuilder.add_node(\"write_memory\", write_memory)\n\n# 添加边 - 简单的线性流程\n# START -> call_model -> write_memory -> END\nbuilder.add_edge(START, \"call_model\")\nbuilder.add_edge(\"call_model\", \"write_memory\")\nbuilder.add_edge(\"write_memory\", END)\n\n# ================== 初始化双记忆系统 ==================\n\n# 长期记忆 (跨线程) - Store\n# - 保存记忆 collection\n# - 每个记忆是独立的条目\n# - 使用 UUID 作为 key\nacross_thread_memory = InMemoryStore()\n\n# 短期记忆 (线程内) - Checkpointer  \n# - 保存对话历史\n# - 每个线程独立\nwithin_thread_memory = MemorySaver()\n\n# ================== 编译 Graph ==================\n\n# LangGraph 知识点:\n# - checkpointer + store: 双记忆架构\n# - 两者协同工作,提供完整的记忆能力\ngraph = builder.compile(\n    checkpointer=within_thread_memory, \n    store=across_thread_memory\n)\n\n# 可视化图结构\ndisplay(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n\n# ================== 总结: Collection Chatbot vs Profile Chatbot ==================\n#\n# 1. Schema 设计:\n#    - Profile: 固定结构,预定义字段 (name, location, interests)\n#    - Collection: 灵活结构,每个记忆独立 (content)\n#\n# 2. 存储方式:\n#    - Profile: 一个 key 保存整个 profile\n#    - Collection: 多个 key,每个保存一个 Memory\n#\n# 3. 更新机制:\n#    - Profile: JSON Patch 更新固定字段\n#    - Collection: 同时支持更新现有记忆和插入新记忆\n#\n# 4. Trustcall 配置:\n#    - Profile: 不需要 enable_inserts\n#    - Collection: 需要 enable_inserts=True\n#\n# 5. 记忆格式化:\n#    - Profile: 格式化为结构化信息 (Name: ..., Location: ...)\n#    - Collection: 格式化为列表 (- memory1, - memory2, ...)\n#\n# 6. 使用场景:\n#    - Profile: 适合结构化的用户信息 (个人档案)\n#    - Collection: 适合开放式的记忆 (对话历史、知识点)\n#\n# 7. 扩展性:\n#    - Profile: 需要预先定义所有字段\n#    - Collection: 可以动态添加任意数量的记忆\n#\n# 8. 查询方式:\n#    - Profile: get(namespace, key)\n#    - Collection: search(namespace)"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, my name is Lance\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Lance! It's great to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Lance\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I like to bike around San Francisco\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That sounds like a lot of fun! San Francisco has some beautiful routes for biking. Do you have a favorite trail or area you like to explore?\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I like to bike around San Francisco\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': {'content': \"User's name is Lance.\"}, 'key': 'dee65880-dd7d-4184-8ca1-1f7400f7596b', 'namespace': ['memories', '1'], 'created_at': '2024-10-30T22:18:52.413283+00:00', 'updated_at': '2024-10-30T22:18:52.413284+00:00'}\n",
      "{'value': {'content': 'User likes to bike around San Francisco.'}, 'key': '662195fc-8ea4-4f64-a6b6-6b86d9cb85c0', 'namespace': ['memories', '1'], 'created_at': '2024-10-30T22:18:56.597813+00:00', 'updated_at': '2024-10-30T22:18:56.597814+00:00'}\n"
     ]
    }
   ],
   "source": [
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace = (\"memories\", user_id)\n",
    "memories = across_thread_memory.search(namespace)\n",
    "for m in memories:\n",
    "    print(m.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I also enjoy going to bakeries\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Biking and bakeries make a great combination! Do you have a favorite bakery in San Francisco, or are you on the hunt for new ones to try?\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I also enjoy going to bakeries\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "在新线程中继续对话。"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What bakeries do you recommend for me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Since you enjoy biking around San Francisco, you might like to check out some of these bakeries that are both delicious and located in areas that are great for a bike ride:\n",
      "\n",
      "1. **Tartine Bakery** - Located in the Mission District, it's famous for its bread and pastries. The area is vibrant and perfect for a leisurely ride.\n",
      "\n",
      "2. **Arsicault Bakery** - Known for its incredible croissants, it's in the Richmond District, which offers a nice ride through Golden Gate Park.\n",
      "\n",
      "3. **B. Patisserie** - Situated in Lower Pacific Heights, this bakery is renowned for its kouign-amann and other French pastries. The neighborhood is charming and bike-friendly.\n",
      "\n",
      "4. **Mr. Holmes Bakehouse** - Famous for its cruffins, it's located in the Tenderloin, which is a bit more urban but still accessible by bike.\n",
      "\n",
      "5. **Noe Valley Bakery** - A cozy spot in Noe Valley, perfect for a stop after exploring the hilly streets of the area.\n",
      "\n",
      "Do any of these sound like a good fit for your next biking adventure?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"What bakeries do you recommend for me?\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### LangSmith \n\nhttps://smith.langchain.com/public/c87543ec-b426-4a82-a3ab-94d01c01d9f4/r\n\n## Studio\n\n![Screenshot 2024-10-30 at 11.29.25 AM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6732d0876d3daa19fef993ba_Screenshot%202024-11-11%20at%207.50.21%E2%80%AFPM.png)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}